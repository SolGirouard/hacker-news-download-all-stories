import urllib2
import json
import datetime
import time
import pytz
import pandas as pd
from pandas import DataFrame

start = datetime.date(2013,1,1)
end = datetime.date(2014,1,1)

iterations = 1268
df = DataFrame()
hitsPerPage = 2
requested_keys = ["id","title","url","points","num_comments","author","created_at_i","text"]

for _ in range(iterations):
   try:
      prefix = 'https://hn.algolia.com/api/v1/'
      query = 'search_by_date?tags=story&hitsPerPage=%s&numericFilters=created_at_i>%s,created_at_i<%s' % (hitsPerPage, start, end)
      url = prefix + query
      print(url)
      req = urllib2.Request(url)
      response = urllib2.urlopen(req)
      data = json.loads(response.read())
      print(data)
      data = DataFrame(data["hits"])[requested_keys]
      print(data)
      df = df.append(data,ignore_index=True)
      print(df)
      ts = data.created_at_i.min()
      time.sleep(3.6)

   except Exception as e:
      print(e)

df["title"] = df["title"].map(lambda x: x.translate(dict.fromkeys([0x201c, 0x201d, 0x2011, 0x2013, 0x2014, 0x2018, 0x2019, 0x2026, 0x2032])).encode('utf-8').replace(',',''))
df["created_at"] = df["created_at_i"].map(lambda x: datetime.datetime.fromtimestamp(int(x), tz=pytz.timezone('America/New_York')).strftime('%Y-%m-%d %H:%M:%S'))

ordered_df = df[["title","url","points","num_comments","author","created_at","objectID"]]

ordered_df.to_csv("hacker_news_stories.csv",encoding='utf-8', index=False)
